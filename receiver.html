<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <script src="//www.gstatic.com/cast/sdk/libs/caf_receiver/v3/cast_receiver_framework.js"></script>
    <style>
        html, body {
            margin: 0; padding: 0;
            width: 100%; height: 100%;
            background: #000; overflow: hidden;
        }
        video {
            width: 100%; height: 100%;
            object-fit: contain;
            background: #000;
        }
        #welcome-img {
            display: none;
            position: fixed; top: 0; left: 0;
            width: 100%; height: 100%;
            object-fit: contain;
            background: #000;
            z-index: 5;
        }
        #status {
            position: fixed; top: 50%; left: 50%;
            transform: translate(-50%, -50%);
            color: #fff; font: 24px -apple-system, sans-serif;
            text-align: center;
            display: none;
            z-index: 10;
            pointer-events: none;
        }
    </style>
</head>
<body>
    <div id="status">Waiting for stream…</div>
    <img id="welcome-img" />
    <video id="v" autoplay playsinline></video>
    <script>
    'use strict';

    const NS  = 'urn:x-cast:com.screenmirror.stream';
    const TAG = '[SMReceiver]';

    const statusEl   = document.getElementById('status');
    const video      = document.getElementById('v');
    const welcomeImg = document.getElementById('welcome-img');

    const ctx = cast.framework.CastReceiverContext.getInstance();
    const pm  = ctx.getPlayerManager();
    let senderBusId = null;
    let soundEnabled = true;

    function report(msg) {
        console.log(TAG, msg);
        if (senderBusId) {
            try { ctx.sendCustomMessage(NS, senderBusId, {type:'DIAG', msg:msg}); } catch(_){}
        }
    }
    function showStatus(text) { statusEl.textContent = text; statusEl.style.display = 'block'; }
    function hideStatus() { statusEl.style.display = 'none'; }

    // ═══════════════════════════════════════════════════════════════
    // Cast Message Handling
    // ═══════════════════════════════════════════════════════════════

    ctx.addCustomMessageListener(NS, event => {
        senderBusId = event.senderId;
        const d = event.data;
        if (d.type === 'WEBRTC_OFFER') {
            report('WEBRTC_OFFER received');
            soundEnabled = d.soundEnabled !== false;
            startWebRTC(d.sdp);
        }
        else if (d.type === 'WEBRTC_ICE') {
            if (pc && d.candidate) {
                pc.addIceCandidate({ candidate: d.candidate, sdpMid: d.sdpMid || '', sdpMLineIndex: d.sdpMLineIndex || 0 })
                  .catch(e => report('addIceCandidate error: ' + e.message));
            }
        }
        else if (d.type === 'STOP_STREAM')  { stopStream(); }
        else if (d.type === 'SHOW_WELCOME') { report('SHOW_WELCOME: ' + d.imageUrl); showWelcome(d.imageUrl); }
        else if (d.type === 'SET_SOUND')    {
            soundEnabled = !!d.enabled;
            if (audioEl) audioEl.muted = !soundEnabled;
            report('SET_SOUND: ' + d.enabled);
        }
    });

    pm.setMessageInterceptor(cast.framework.messages.MessageType.LOAD, req => {
        return null;
    });

    const opts = new cast.framework.CastReceiverOptions();
    opts.disableIdleTimeout = true;
    opts.statusText = '';
    opts.customNamespaces = {};
    opts.customNamespaces[NS] = cast.framework.system.MessageType.JSON;
    ctx.start(opts);
    report('Receiver v8 — WebRTC Video Track (SRTP/UDP) + DataChannel Audio');

    // ═══════════════════════════════════════════════════════════════
    // State
    // ═══════════════════════════════════════════════════════════════

    let pc = null;           // RTCPeerConnection
    let dc = null;           // RTCDataChannel (audio)

    // Audio MSE state
    let audioEl = null, audioMS = null, audioSB = null;
    let audioQueue = [], audioAppending = false, audioInitReceived = false;
    let lastAudioInitSeg = null;
    let lastObjectURL = null;

    let hasPlayed = false;
    let diagTimer = null, liveTimer = null;
    let streamStartTime = 0;
    let totalBytes = 0, fragments = 0;

    // ═══════════════════════════════════════════════════════════════
    // Welcome Image
    // ═══════════════════════════════════════════════════════════════

    function showWelcome(imageUrl) {
        stopStream();
        welcomeImg.src = imageUrl;
        welcomeImg.style.display = 'block';
        video.style.display = 'none';
        hideStatus();
    }

    // ═══════════════════════════════════════════════════════════════
    // WebRTC: Native Video Track + DataChannel Audio
    // ═══════════════════════════════════════════════════════════════

    function startWebRTC(offerSDP) {
        welcomeImg.style.display = 'none';
        video.style.display = 'block';
        showStatus('Connecting…');
        stopStream();

        streamStartTime = Date.now();
        report('Starting WebRTC (native video track mode)');

        const config = {
            iceServers: [],
            bundlePolicy: 'max-bundle',
            rtcpMuxPolicy: 'require',
            sdpSemantics: 'unified-plan'
        };

        try {
            pc = new RTCPeerConnection(config);
        } catch(e) {
            report('RTCPeerConnection failed: ' + e.message);
            sendUnsupported();
            return;
        }

        // ── Video: native WebRTC track → video.srcObject ──
        pc.ontrack = (event) => {
            const kind = event.track.kind;
            report('Track received: kind=' + kind + ' id=' + event.track.id);
            if (kind === 'video') {
                video.srcObject = event.streams[0] || new MediaStream([event.track]);
                video.muted = true;  // Audio comes from DataChannel, not video element
                video.play().then(() => {
                    hasPlayed = true;
                    hideStatus();
                    report('VIDEO PLAYING via native WebRTC track');
                }).catch(e => {
                    report('play error: ' + e.message);
                    video.muted = true;
                    video.play().then(() => {
                        hasPlayed = true;
                        hideStatus();
                        report('PLAYING (muted retry)');
                    }).catch(() => {});
                });
            }
        };

        // ── Audio: DataChannel → fMP4 MSE (hidden element) ──
        pc.ondatachannel = (event) => {
            dc = event.channel;
            dc.binaryType = 'arraybuffer';
            report('DataChannel opened: ' + dc.label + ' ordered=' + dc.ordered);

            dc.onmessage = (evt) => {
                const data = new Uint8Array(evt.data);
                totalBytes += data.length;
                fragments++;

                if (!audioInitReceived && isFtyp(data)) {
                    audioInitReceived = true;
                    lastAudioInitSeg = data;
                    report('Audio init segment: ' + data.length + ' bytes');
                    setupAudioMSE(data);
                    return;
                }

                // Re-init on new init segment (resolution change)
                if (isFtyp(data)) {
                    lastAudioInitSeg = data;
                    report('Audio re-init: ' + data.length + ' bytes');
                    reinitAudioMSE();
                    return;
                }

                audioEnqueue(data);
            };

            dc.onclose = () => report('DataChannel closed');
            dc.onerror = (e) => report('DataChannel error: ' + (e.error ? e.error.message : '?'));
        };

        // ICE candidates → relay to phone via Cast namespace
        pc.onicecandidate = (event) => {
            if (event.candidate && senderBusId) {
                try {
                    ctx.sendCustomMessage(NS, senderBusId, {
                        type: 'WEBRTC_ICE',
                        candidate: event.candidate.candidate,
                        sdpMid: event.candidate.sdpMid,
                        sdpMLineIndex: event.candidate.sdpMLineIndex
                    });
                } catch(_) {}
            }
        };

        pc.oniceconnectionstatechange = () => {
            const st = pc.iceConnectionState;
            report('ICE state: ' + st);
            if (st === 'connected' || st === 'completed') {
                showStatus('Receiving…');
            } else if (st === 'failed') {
                report('ICE connection failed');
                showStatus('Connection failed');
            } else if (st === 'disconnected') {
                report('ICE disconnected — waiting for recovery');
            }
        };

        // SDP exchange: set remote offer → create answer → send back
        pc.setRemoteDescription({ type: 'offer', sdp: offerSDP })
          .then(() => pc.createAnswer())
          .then(answer => {
              let sdp = answer.sdp;
              if (sdp.indexOf('a=max-message-size:') !== -1) {
                  sdp = sdp.replace(/a=max-message-size:\d+/, 'a=max-message-size:262144');
              }
              return pc.setLocalDescription({ type: 'answer', sdp: sdp });
          })
          .then(() => {
              report('WebRTC answer ready, sending to phone');
              try {
                  ctx.sendCustomMessage(NS, senderBusId, {
                      type: 'WEBRTC_ANSWER',
                      sdp: pc.localDescription.sdp
                  });
              } catch(e) {
                  report('Failed to send WEBRTC_ANSWER: ' + e.message);
              }
          })
          .catch(e => {
              report('WebRTC setup failed: ' + e.message);
              sendUnsupported();
          });

        startDiag();
        startLiveLoop();
    }

    function sendUnsupported() {
        if (senderBusId) {
            try { ctx.sendCustomMessage(NS, senderBusId, { type: 'WEBRTC_UNSUPPORTED' }); } catch(_) {}
        }
    }

    function isFtyp(d) {
        return d.length > 7 && d[4]===0x66 && d[5]===0x74 && d[6]===0x79 && d[7]===0x70;
    }

    // ═══════════════════════════════════════════════════════════════
    // Audio MSE Pipeline (DataChannel → hidden SourceBuffer)
    // ═══════════════════════════════════════════════════════════════

    function setupAudioMSE(initSeg) {
        cleanupAudio();

        audioEl = document.createElement('video');
        audioEl.style.cssText = 'position:fixed;width:1px;height:1px;opacity:0;pointer-events:none';
        audioEl.muted = !soundEnabled;
        audioEl.autoplay = true;
        document.body.appendChild(audioEl);

        audioMS = new MediaSource();
        audioMS.addEventListener('sourceopen', () => {
            const codec = detectCodec(initSeg) || 'avc1.640029,mp4a.40.2';
            report('Audio MSE codec: ' + codec);

            try {
                audioSB = audioMS.addSourceBuffer('video/mp4; codecs="' + codec + '"');
            } catch(e) {
                report('Audio addSourceBuffer failed: ' + e.message);
                try {
                    audioSB = audioMS.addSourceBuffer('video/mp4; codecs="mp4a.40.2"');
                } catch(e2) {
                    report('Audio codec fallback failed: ' + e2.message);
                    return;
                }
            }

            audioSB.mode = 'sequence';
            audioSB.addEventListener('updateend', () => {
                audioAppending = false;
                audioDrainQueue();
            });
            audioSB.addEventListener('error', () => {
                report('Audio SourceBuffer error');
                audioQueue = [];
                audioAppending = false;
            });

            audioQueue.unshift(initSeg);
            audioDrainQueue();

            // Start audio playback after brief buffer
            setTimeout(() => {
                if (audioEl && audioSB && audioSB.buffered.length > 0) {
                    const edge = audioSB.buffered.end(audioSB.buffered.length - 1);
                    audioEl.currentTime = Math.max(edge - 0.1, 0);
                    audioEl.play().catch(() => {});
                }
            }, 500);
        });

        audioMS.addEventListener('sourceclose', () => report('Audio MSE sourceclose'));

        lastObjectURL = URL.createObjectURL(audioMS);
        audioEl.src = lastObjectURL;
    }

    function reinitAudioMSE() {
        if (lastAudioInitSeg) setupAudioMSE(lastAudioInitSeg);
    }

    function audioEnqueue(data) {
        if (!audioSB || !audioMS || audioMS.readyState !== 'open') return;
        if (audioQueue.length >= 30) {
            audioQueue.splice(0, audioQueue.length - 10);
        }
        audioQueue.push(data);
        audioDrainQueue();
    }

    function audioDrainQueue() {
        if (audioAppending || !audioSB || audioQueue.length === 0) return;
        if (audioSB.updating) return;
        if (!audioMS || audioMS.readyState !== 'open') { audioQueue = []; return; }

        audioAppending = true;
        try {
            audioSB.appendBuffer(audioQueue.shift());
        } catch(e) {
            audioAppending = false;
            if (e.name === 'QuotaExceededError') {
                audioTrimBuffer();
            } else if (e.name === 'InvalidStateError') {
                audioQueue = [];
            }
        }
    }

    function audioTrimBuffer() {
        if (!audioSB || audioSB.buffered.length === 0 || audioSB.updating) return;
        try {
            const s = audioSB.buffered.start(0);
            const keep = Math.max(s, (audioEl ? audioEl.currentTime : 0) - 2.0);
            if (keep > s + 0.5) audioSB.remove(s, keep);
        } catch(_) {}
    }

    function cleanupAudio() {
        if (audioMS && audioMS.readyState === 'open') {
            try { audioMS.endOfStream(); } catch(_) {}
        }
        if (lastObjectURL) { URL.revokeObjectURL(lastObjectURL); lastObjectURL = null; }
        if (audioEl) { audioEl.remove(); audioEl = null; }
        audioMS = null;
        audioSB = null;
        audioQueue = [];
        audioAppending = false;
    }

    // ═══════════════════════════════════════════════════════════════
    // Stream Control
    // ═══════════════════════════════════════════════════════════════

    function stopStream() {
        if (dc) { try { dc.close(); } catch(_) {} dc = null; }
        if (pc) { try { pc.close(); } catch(_) {} pc = null; }
        if (diagTimer) { clearInterval(diagTimer); diagTimer = null; }
        if (liveTimer) { clearInterval(liveTimer); liveTimer = null; }
        cleanupAudio();
        video.srcObject = null;
        audioInitReceived = false;
        hasPlayed = false;
        totalBytes = 0;
        fragments = 0;
    }

    // ═══════════════════════════════════════════════════════════════
    // Video Event Handlers (minimal — WebRTC handles most things)
    // ═══════════════════════════════════════════════════════════════

    video.addEventListener('playing', () => {
        hideStatus();
        if (!hasPlayed) hasPlayed = true;
    });

    video.addEventListener('error', () => {
        const err = video.error;
        report('video error: code=' + (err ? err.code : '?'));
    });

    // ═══════════════════════════════════════════════════════════════
    // Live Loop & Diagnostics
    // ═══════════════════════════════════════════════════════════════

    function startLiveLoop() {
        if (liveTimer) clearInterval(liveTimer);
        liveTimer = setInterval(() => {
            // Keep audio near live edge
            if (audioEl && audioSB && audioSB.buffered.length > 0) {
                const edge = audioSB.buffered.end(audioSB.buffered.length - 1);
                const drift = edge - audioEl.currentTime;
                if (drift > 1.0) {
                    audioEl.currentTime = edge - 0.2;
                }
                if (audioEl.paused) audioEl.play().catch(() => {});

                // Trim old audio buffer
                if (!audioSB.updating) {
                    const s = audioSB.buffered.start(0);
                    const rm = audioEl.currentTime - 3.0;
                    if (rm > s + 0.5) {
                        try { audioSB.remove(s, rm); } catch(_) {}
                    }
                }
            }
        }, 200);
    }

    function startDiag() {
        if (diagTimer) clearInterval(diagTimer);
        diagTimer = setInterval(() => {
            const elapsed = ((Date.now() - streamStartTime) / 1000).toFixed(1);
            const iceState = pc ? pc.iceConnectionState : 'none';

            let videoInfo = 'no-track';
            if (video.srcObject) {
                const vt = video.srcObject.getVideoTracks();
                if (vt.length > 0) {
                    const s = vt[0].getSettings ? vt[0].getSettings() : {};
                    videoInfo = (s.width || '?') + 'x' + (s.height || '?')
                        + '@' + (s.frameRate ? s.frameRate.toFixed(0) : '?') + 'fps';
                }
            }

            let dropInfo = '';
            if (video.getVideoPlaybackQuality) {
                const q = video.getVideoPlaybackQuality();
                dropInfo = ' drop=' + q.droppedVideoFrames + '/' + q.totalVideoFrames;
            }

            let audioInfo = 'none';
            if (audioSB && audioSB.buffered.length > 0) {
                const be = audioSB.buffered.end(audioSB.buffered.length - 1).toFixed(2);
                const ct = audioEl ? audioEl.currentTime.toFixed(2) : '?';
                audioInfo = 'ct=' + ct + ' edge=' + be;
            }

            const kbps = streamStartTime > 0
                ? ((totalBytes * 8) / ((Date.now() - streamStartTime) / 1000) / 1000).toFixed(0)
                : '0';

            report('DIAG t=' + elapsed + 's ice=' + iceState
                + ' video=[' + videoInfo + ']' + dropInfo
                + ' audio=[' + audioInfo + ']'
                + ' frags=' + fragments + ' ' + kbps + 'kbps');
        }, 5000);
    }

    function detectCodec(d) {
        let videoCodec = null, hasAudio = false;
        for (let i = 0; i < d.length - 4; i++) {
            if (!videoCodec && i + 7 < d.length &&
                d[i]===0x61 && d[i+1]===0x76 && d[i+2]===0x63 && d[i+3]===0x43) {
                if (d[i+4] === 1) {
                    const h = n => n.toString(16).padStart(2, '0');
                    videoCodec = 'avc1.' + h(d[i+5]) + h(d[i+6]) + h(d[i+7]);
                }
            }
            if (d[i]===0x6D && d[i+1]===0x70 && d[i+2]===0x34 && d[i+3]===0x61) hasAudio = true;
            if (d[i]===0x65 && d[i+1]===0x73 && d[i+2]===0x64 && d[i+3]===0x73) hasAudio = true;
        }
        if (videoCodec && (hasAudio || d.length > 800)) return videoCodec + ',mp4a.40.2';
        return videoCodec;
    }
    </script>
</body>
</html>
